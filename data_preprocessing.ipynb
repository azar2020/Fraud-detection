{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a2b1dc-6201-4887-8a87-047c69d9864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''I used this code to figure out the encoding of the files so I can combine the yearly ones! \n",
    "\n",
    "Didnt let it run this time since I already did before'''\n",
    "\n",
    "import chardet\n",
    "import pandas as pd\n",
    "\n",
    "def read_csv_robust(file_path):\n",
    "    # Guess the encoding of the file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        result = chardet.detect(file.read(100000))  # Read the first 100000 bytes to guess encoding\n",
    "        encoding = result['encoding']\n",
    "        confidence = result['confidence']\n",
    "        print(f\"Detected encoding {encoding} with confidence {confidence} for file {file_path}\")\n",
    "\n",
    "    # Handle common misdetected encodings\n",
    "    if encoding == 'ascii' or confidence < 0.5:\n",
    "        encoding = 'utf-8'  # Default to UTF-8 if ASCII is detected or low confidence\n",
    "\n",
    "    # Now read the file with the detected encoding\n",
    "    try:\n",
    "        return pd.read_csv(file_path, encoding=encoding), encoding\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback to latin1 if UTF-8 fails\n",
    "        return pd.read_csv(file_path, encoding='latin1'), 'latin1'\n",
    "\n",
    "# List your CSV files here\n",
    "csv_files = [\n",
    "    'MUP_DPR_RY24_P04_V10_DY13_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY14_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY15_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY16_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY17_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY18_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY19_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY20_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY21_NPIBN.csv',\n",
    "    'MUP_DPR_RY24_P04_V10_DY22_NPIBN.csv'\n",
    "]\n",
    "\n",
    "file_encodings = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        current_df, encoding = read_csv_robust(file)\n",
    "        file_encodings[file] = encoding  # Store the successful encoding\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with {file}: {e}\")\n",
    "        break\n",
    "\n",
    "print(\"File encodings:\", file_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176963c-2424-4bec-9a83-f631f4bff2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing MUP_DPR_RY24_P04_V10_DY13_NPIBN.csv: \"['Prscrbr_State_Abrvtn', 'Prscrbr_State_FIPS', 'Prscrbr_Type_Src', 'Prscrbr_City'] not found in axis\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY14_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY15_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY16_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY17_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY18_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved MUP_DPR_RY24_P04_V10_DY19_NPIBN.csv successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4551/4037098797.py:24: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file, encoding=encoding)\n"
     ]
    }
   ],
   "source": [
    "\"\"\" This code filters out the columns before merging the large dataset for efficacy.\n",
    "\n",
    "First file I process already thats why it gives error saying the columns not found just ignore that part\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths and encodings\n",
    "file_encodings = {\n",
    "    'MUP_DPR_RY24_P04_V10_DY13_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY14_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY15_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY16_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY17_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY18_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY19_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY20_NPIBN.csv': 'latin1',  # Different encoding\n",
    "    'MUP_DPR_RY24_P04_V10_DY21_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY22_NPIBN.csv': 'utf-8'\n",
    "}\n",
    "\n",
    "# Process each file\n",
    "for file, encoding in file_encodings.items():\n",
    "    try:\n",
    "        # Read the CSV file with specified encoding\n",
    "        df = pd.read_csv(file, encoding=encoding)\n",
    "\n",
    "        # Drop the specified columns\n",
    "        df = df.drop(columns=[\n",
    "            #'Prscrbr_Last_Org_Name', 'Prscrbr_First_Name',  ---> Already removed these beforehand\n",
    "            'Prscrbr_State_Abrvtn', 'Prscrbr_State_FIPS',\n",
    "            'Prscrbr_Type_Src', 'Prscrbr_City'\n",
    "        ])\n",
    "\n",
    "        # Save the modified DataFrame back to the same CSV file\n",
    "        df.to_csv(file, index=False, encoding=encoding)\n",
    "        print(f\"Processed and saved {file} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "628eaa6e-dd93-4f00-9fdb-12c96c7895b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files combined successfully into 'combined_data.csv' with 'Year' column.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"In this code we combine everything into the large csv. \"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File paths and encodings\n",
    "file_encodings = {\n",
    "    'MUP_DPR_RY24_P04_V10_DY13_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY14_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY15_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY16_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY17_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY18_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY19_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY20_NPIBN.csv': 'latin1',  # Different encoding\n",
    "    'MUP_DPR_RY24_P04_V10_DY21_NPIBN.csv': 'utf-8',\n",
    "    'MUP_DPR_RY24_P04_V10_DY22_NPIBN.csv': 'utf-8'\n",
    "}\n",
    "\n",
    "dataframes = []\n",
    "reference_schema = None\n",
    "schema_mismatch = False\n",
    "\n",
    "# Read data and check schema consistency\n",
    "for file, encoding in file_encodings.items():\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding=encoding, low_memory=False)\n",
    "        # Extract year from filename and add it as a new column\n",
    "        year = int(file.split('_DY')[1][:2]) + 2000  # Extracting and converting DYXX to a full year\n",
    "        df['Year'] = year\n",
    "        if reference_schema is None:\n",
    "            reference_schema = df.columns.tolist()\n",
    "        elif not all(x == y for x, y in zip(reference_schema, df.columns.tolist())):\n",
    "            schema_mismatch = True\n",
    "            print(f\"Schema mismatch found in file: {file}\")\n",
    "            break\n",
    "        dataframes.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file}: {e}\")\n",
    "        break\n",
    "\n",
    "# Concatenate all dataframes if no schema mismatch\n",
    "if not schema_mismatch:\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df.to_csv('combined_filtered_data.csv', index=False)\n",
    "    print(\"All files combined successfully into 'combined_data.csv' with 'Year' column.\")\n",
    "else:\n",
    "    print(\"Files were not combined due to schema mismatches.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ce6c93e-337e-4172-bc9c-3dc2162418dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the first 10 rows to 'first_10_rows.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the first 10 rows of the CSV file into a DataFrame\n",
    "df = pd.read_csv('../Data/combined_filtered_data.csv', nrows=10)\n",
    "\n",
    "# Save the DataFrame with the first 10 rows to a new CSV file\n",
    "df.to_csv('../Data/first_10_rows.csv', index=False)\n",
    "\n",
    "print(\"Saved the first 10 rows to 'first_10_rows.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef2e90e-b8f5-4843-821f-4d78c2581e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of GE65_Tot_Benes suppressed with non-empty GE65_Bene_Sprsn_Flag: 207690783\n",
      "Count of GE65_Tot_Benes suppressed with empty GE65_Bene_Sprsn_Flag: 0\n"
     ]
    }
   ],
   "source": [
    "# Tell us how many of GE65_Tot_Benes are empty on purpose. \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to your dataset\n",
    "file_path = '../Data/combined_filtered_data.csv'\n",
    "\n",
    "# Initialize counters\n",
    "count_suppressed_with_flag = 0\n",
    "count_suppressed_without_flag = 0\n",
    "\n",
    "# Process the dataset in chunks (for example, 1 million rows at a time)\n",
    "chunksize = 10**6  # Adjust based on your system's memory\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    # Count instances where GE65_Tot_Benes is suppressed and GE65_Bene_Sprsn_Flag is not empty\n",
    "    count_suppressed_with_flag += ((chunk['GE65_Tot_Benes'].isna() | (chunk['GE65_Tot_Benes'] == '')) & (chunk['GE65_Bene_Sprsn_Flag'] != '')).sum()\n",
    "\n",
    "    # Count instances where GE65_Tot_Benes is suppressed but GE65_Bene_Sprsn_Flag is empty\n",
    "    count_suppressed_without_flag += ((chunk['GE65_Tot_Benes'].isna() | (chunk['GE65_Tot_Benes'] == '')) & (chunk['GE65_Bene_Sprsn_Flag'] == '')).sum()\n",
    "\n",
    "# Printing counts\n",
    "print(\"Count of GE65_Tot_Benes suppressed with non-empty GE65_Bene_Sprsn_Flag:\", count_suppressed_with_flag)\n",
    "print(\"Count of GE65_Tot_Benes suppressed with empty GE65_Bene_Sprsn_Flag:\", count_suppressed_without_flag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de72e204-5b73-49b1-be33-e49a60651a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total instances with suppressed values and a non-empty flag: 401188316\n",
      "Total instances with suppressed values but no flag (potentially unintentional): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = '../Data/combined_filtered_data.csv'\n",
    "\n",
    "# Initialize counters\n",
    "count_suppressed_with_flag = 0\n",
    "count_suppressed_without_flag = 0\n",
    "\n",
    "# Process the dataset in chunks (for example, 1 million rows at a time)\n",
    "chunksize = 10**6  # Modify this based on your memory availability\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize):\n",
    "    # Loop through each variable of interest\n",
    "    for var in ['GE65_Tot_Clms', 'GE65_Tot_30day_Fills', 'GE65_Tot_Drug_Cst', 'GE65_Tot_Day_Suply']:\n",
    "        # Sum instances where the variable is suppressed and a flag is present\n",
    "        count_suppressed_with_flag += ((chunk[var].isna() | (chunk[var] == '')) & (chunk['GE65_Sprsn_Flag'] != '')).sum()\n",
    "        # Sum instances where the variable is suppressed and no flag is present\n",
    "        count_suppressed_without_flag += ((chunk[var].isna() | (chunk[var] == '')) & (chunk['GE65_Sprsn_Flag'] == '')).sum()\n",
    "\n",
    "# Print results after processing all chunks\n",
    "print(f\"Total instances with suppressed values and a non-empty flag: {count_suppressed_with_flag}\")\n",
    "print(f\"Total instances with suppressed values but no flag (potentially unintentional): {count_suppressed_without_flag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368ce4cb-30c1-44ad-88bb-84fb7039139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Data Analysis Report:\n",
      "\n",
      "Column: Prscrbr_NPI\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Prscrbr_Type\n",
      "Total Null Entries: 83\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Brnd_Name\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Gnrc_Name\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Tot_Clms\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Tot_30day_Fills\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Tot_Day_Suply\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Tot_Drug_Cst\n",
      "Total Null Entries: 0\n",
      "Percentage of Total Entries: 0.00%\n",
      "\n",
      "Column: Tot_Benes\n",
      "Total Null Entries: 136634558\n",
      "Percentage of Total Entries: 59.34%\n",
      "\n",
      "Column: GE65_Sprsn_Flag\n",
      "Total Null Entries: 129952695\n",
      "Percentage of Total Entries: 56.44%\n",
      "\n",
      "Column: GE65_Tot_Clms\n",
      "Total Null Entries: 100297079\n",
      "Percentage of Total Entries: 43.56%\n",
      "\n",
      "Column: GE65_Tot_30day_Fills\n",
      "Total Null Entries: 100297079\n",
      "Percentage of Total Entries: 43.56%\n",
      "\n",
      "Column: GE65_Tot_Drug_Cst\n",
      "Total Null Entries: 100297079\n",
      "Percentage of Total Entries: 43.56%\n",
      "\n",
      "Column: GE65_Tot_Day_Suply\n",
      "Total Null Entries: 100297079\n",
      "Percentage of Total Entries: 43.56%\n",
      "\n",
      "Column: GE65_Bene_Sprsn_Flag\n",
      "Total Null Entries: 22558991\n",
      "Percentage of Total Entries: 9.80%\n",
      "\n",
      "Column: GE65_Tot_Benes\n",
      "Total Null Entries: 207690783\n",
      "Percentage of Total Entries: 90.20%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to your dataset\n",
    "file_path = '../Data/combined_filtered_data.csv'\n",
    "\n",
    "# List of columns to check for null values\n",
    "columns_to_check = [\n",
    "    'Prscrbr_NPI', 'Prscrbr_Type', 'Brnd_Name', 'Gnrc_Name', 'Tot_Clms',\n",
    "    'Tot_30day_Fills', 'Tot_Day_Suply', 'Tot_Drug_Cst', 'Tot_Benes',\n",
    "    'GE65_Sprsn_Flag', 'GE65_Tot_Clms', 'GE65_Tot_30day_Fills', 'GE65_Tot_Drug_Cst',\n",
    "    'GE65_Tot_Day_Suply', 'GE65_Bene_Sprsn_Flag', 'GE65_Tot_Benes'\n",
    "]\n",
    "\n",
    "# Initialize a dictionary to hold total counts of nulls\n",
    "null_counts = {column: 0 for column in columns_to_check}\n",
    "\n",
    "# Initialize a variable to hold the total number of rows processed\n",
    "total_rows = 0\n",
    "\n",
    "# Process the dataset in chunks\n",
    "chunksize = 10**6  # Adjust based on your system's memory\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunksize, usecols=columns_to_check):\n",
    "    total_rows += len(chunk)\n",
    "    for column in columns_to_check:\n",
    "        # Count nulls in the current chunk for each column\n",
    "        null_counts[column] += chunk[column].isna().sum()\n",
    "\n",
    "# Printing results with a description\n",
    "print(\"Null Data Analysis Report:\")\n",
    "for column in columns_to_check:\n",
    "    null_percentage = (null_counts[column] / total_rows) * 100\n",
    "    print(f\"\\nColumn: {column}\")\n",
    "    print(f\"Total Null Entries: {null_counts[column]}\")\n",
    "    print(f\"Percentage of Total Entries: {null_percentage:.2f}%\")\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb75abd-f004-4ce9-b3e2-ad19c3165cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
